{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wf7vi9VKPUL"
      },
      "source": [
        "# **Importaci√≥n de librer√≠as y carga de datos**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apeXpdOE6oHk",
        "outputId": "cc6c88da-0a5a-44e9-e611-d8f4e3a86b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "fj1GMgGF-u34",
        "outputId": "5ff0793f-6d18-4af5-fd93-7815f2b2f2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Cargando datos...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mensajes.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1827508749.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Carga de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cargando datos...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdf_mensajes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mensajes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total de mensajes cargados: {len(df_mensajes)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Columnas: {df_mensajes.columns.tolist()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mensajes.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "import json\n",
        "from collections import Counter\n",
        "import unicodedata\n",
        "from fuzzywuzzy import fuzz, process\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Para procesamiento de texto\n",
        "import unicodedata\n",
        "from difflib import SequenceMatcher\n",
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz, process\n",
        "\n",
        "# Carga de datos\n",
        "print(\"Cargando datos...\")\n",
        "df_mensajes = pd.read_csv('mensajes.csv')\n",
        "print(f\"Total de mensajes cargados: {len(df_mensajes)}\")\n",
        "print(f\"Columnas: {df_mensajes.columns.tolist()}\")\n",
        "print(df_mensajes.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUBMlxNuKX0L"
      },
      "source": [
        "# **Limpieza de texto**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "lpc48GDcCKpk",
        "outputId": "26be07f5-6d45-4404-9dc9-4dcfcd8fa21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Limpiando textos...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_mensajes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2219344080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Aplicar limpieza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLimpiando textos...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdf_mensajes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contenido_limpio'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_mensajes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contenido'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimpiar_texto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEliminando duplicados...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_mensajes' is not defined"
          ]
        }
      ],
      "source": [
        "def limpiar_texto(texto):\n",
        "    \"\"\"\n",
        "    Limpia el texto eliminando emojis, URLs y caracteres especiales\n",
        "    \"\"\"\n",
        "    if pd.isna(texto):\n",
        "        return \"\"\n",
        "\n",
        "    # Convertir a string\n",
        "    texto = str(texto)\n",
        "\n",
        "    # Eliminar URLs\n",
        "    texto = re.sub(r'http\\S+|www.\\S+', '', texto)\n",
        "\n",
        "    # Eliminar emojis y caracteres especiales\n",
        "    texto = re.sub(r'[^\\w\\s\\,\\.\\:\\;\\-\\$\\%\\/\\(\\)]', ' ', texto)\n",
        "\n",
        "    # Eliminar m√∫ltiples espacios\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "\n",
        "    # Eliminar espacios al inicio y final\n",
        "    texto = texto.strip()\n",
        "\n",
        "    return texto\n",
        "\n",
        "def eliminar_duplicados(df):\n",
        "    \"\"\"\n",
        "    Elimina mensajes duplicados bas√°ndose en el contenido\n",
        "    \"\"\"\n",
        "    # Crear columna temporal con texto limpio para comparaci√≥n\n",
        "    df['contenido_limpio_temp'] = df['contenido'].apply(limpiar_texto)\n",
        "\n",
        "    # Eliminar duplicados exactos\n",
        "    df_sin_dup = df.drop_duplicates(subset=['contenido_limpio_temp'], keep='first')\n",
        "\n",
        "    # Eliminar la columna temporal\n",
        "    df_sin_dup = df_sin_dup.drop('contenido_limpio_temp', axis=1)\n",
        "\n",
        "    print(f\"Mensajes originales: {len(df)}\")\n",
        "    print(f\"Mensajes despu√©s de eliminar duplicados: {len(df_sin_dup)}\")\n",
        "\n",
        "    return df_sin_dup\n",
        "\n",
        "# Aplicar limpieza\n",
        "print(\"\\nLimpiando textos...\")\n",
        "df_mensajes['contenido_limpio'] = df_mensajes['contenido'].apply(limpiar_texto)\n",
        "\n",
        "print(\"\\nEliminando duplicados...\")\n",
        "df_limpio = eliminar_duplicados(df_mensajes)\n",
        "\n",
        "# Filtrar mensajes con contenido relevante (m√°s de 50 caracteres)\n",
        "df_limpio = df_limpio[df_limpio['contenido_limpio'].str.len() > 50].copy()\n",
        "print(f\"Mensajes con contenido relevante: {len(df_limpio)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7lBoJ8-KdMn"
      },
      "source": [
        "# **Extracci√≥n de informaci√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgPyQvctCTAY"
      },
      "outputs": [],
      "source": [
        "def extraer_informacion_oferta(texto):\n",
        "    \"\"\"\n",
        "    Extrae informaci√≥n estructurada de la oferta laboral\n",
        "    \"\"\"\n",
        "    info = {\n",
        "        'cargo': None,\n",
        "        'empresa': None,\n",
        "        'ubicacion': None,\n",
        "        'salario': None,\n",
        "        'tipo_contrato': None,\n",
        "        'nivel_educativo': None,\n",
        "        'experiencia': None,\n",
        "        'vacantes': None,\n",
        "        'fecha_evento': None,\n",
        "        'lugar_evento': None\n",
        "    }\n",
        "\n",
        "    # Extraer cargo (buscar despu√©s de palabras clave)\n",
        "    cargo_patterns = [\n",
        "        r'vacante[s]?\\s+(?:para\\s+)?([^\\n\\r\\.]+)',\n",
        "        r'busca[n]?\\s+(\\d+)?\\s*([^\\n\\r\\.]+?)(?:\\s+con|\\s+Tipo|\\s+Nivel|\\s+Experiencia)',\n",
        "        r'puestos?\\s+de\\s+trabajo\\s+(?:para|como)?\\s*([^\\n\\r\\.]+)',\n",
        "        r'cargo[s]?:?\\s*([^\\n\\r\\.]+)',\n",
        "    ]\n",
        "\n",
        "    for pattern in cargo_patterns:\n",
        "        match = re.search(pattern, texto, re.IGNORECASE)\n",
        "        if match:\n",
        "            if match.lastindex >= 2:\n",
        "                info['cargo'] = match.group(2).strip()\n",
        "            else:\n",
        "                info['cargo'] = match.group(1).strip()\n",
        "            break\n",
        "\n",
        "    # Extraer empresa\n",
        "    empresa_patterns = [\n",
        "        r'([A-Z][a-zA-Z\\s]+(?:S\\.A\\.S|SAS|S\\.A|SA|Ltda|LTDA))',\n",
        "        r'(?:empresa|compa√±√≠a):\\s*([^\\n\\r\\.]+)',\n",
        "    ]\n",
        "\n",
        "    for pattern in empresa_patterns:\n",
        "        match = re.search(pattern, texto)\n",
        "        if match:\n",
        "            info['empresa'] = match.group(1).strip()\n",
        "            break\n",
        "\n",
        "    # Extraer ubicaci√≥n/ciudad\n",
        "    ciudades = ['Bogot√°', 'Medell√≠n', 'Cali', 'Barranquilla', 'Cartagena',\n",
        "                'Suba', 'Kennedy', 'Fontib√≥n', 'Chapinero', 'Bosa', 'Usaqu√©n']\n",
        "\n",
        "    for ciudad in ciudades:\n",
        "        if ciudad.lower() in texto.lower():\n",
        "            info['ubicacion'] = ciudad\n",
        "            break\n",
        "\n",
        "    # Extraer salario\n",
        "    salario_match = re.search(r'\\$\\s*[\\d\\.,]+', texto)\n",
        "    if salario_match:\n",
        "        info['salario'] = salario_match.group(0).strip()\n",
        "\n",
        "    # Extraer tipo de contrato\n",
        "    contratos = ['indefinido', 't√©rmino fijo', 'termino fijo', 'obra labor', 'prestaci√≥n de servicios']\n",
        "    for contrato in contratos:\n",
        "        if contrato in texto.lower():\n",
        "            info['tipo_contrato'] = contrato.title()\n",
        "            break\n",
        "\n",
        "    # Extraer nivel educativo\n",
        "    niveles = ['bachiller', 't√©cnico', 'tecn√≥logo', 'profesional', 'especializaci√≥n', 'maestr√≠a']\n",
        "    for nivel in niveles:\n",
        "        if nivel in texto.lower():\n",
        "            info['nivel_educativo'] = nivel.title()\n",
        "            break\n",
        "\n",
        "    # Extraer experiencia\n",
        "    exp_match = re.search(r'(\\d+)\\s*(?:mes|a√±o)[s]?\\s+(?:de\\s+)?experiencia', texto, re.IGNORECASE)\n",
        "    if exp_match:\n",
        "        info['experiencia'] = exp_match.group(0).strip()\n",
        "    elif 'sin experiencia' in texto.lower() or 'no requiere experiencia' in texto.lower():\n",
        "        info['experiencia'] = 'No requiere'\n",
        "\n",
        "    # Extraer n√∫mero de vacantes\n",
        "    vacantes_match = re.search(r'(\\d+)\\s+(?:puestos?|vacantes?)', texto, re.IGNORECASE)\n",
        "    if vacantes_match:\n",
        "        info['vacantes'] = int(vacantes_match.group(1))\n",
        "\n",
        "    # Extraer fecha de evento\n",
        "    fecha_match = re.search(r'(\\d{1,2})\\s+de\\s+(\\w+)', texto, re.IGNORECASE)\n",
        "    if fecha_match:\n",
        "        info['fecha_evento'] = fecha_match.group(0).strip()\n",
        "\n",
        "    # Extraer lugar del evento\n",
        "    lugar_patterns = [\n",
        "        r'(?:Lugar|D√≥nde|Donde):\\s*([^\\n\\r]+?)(?:\\n|\\r|Fecha|Hora|$)',\n",
        "        r'(?:Centro Comercial|C\\.C\\.|Calle|Carrera|Av\\.|Avenida)\\s+([^\\n\\r]+?)(?:\\n|\\r|Fecha|$)'\n",
        "    ]\n",
        "\n",
        "    for pattern in lugar_patterns:\n",
        "        match = re.search(pattern, texto, re.IGNORECASE)\n",
        "        if match:\n",
        "            info['lugar_evento'] = match.group(1).strip()[:100]\n",
        "            break\n",
        "\n",
        "    return info\n",
        "\n",
        "# Aplicar extracci√≥n\n",
        "print(\"\\nExtrayendo informaci√≥n de ofertas...\")\n",
        "info_extraida = df_limpio['contenido_limpio'].apply(extraer_informacion_oferta)\n",
        "df_info = pd.DataFrame(info_extraida.tolist())\n",
        "\n",
        "# Combinar con el dataframe original\n",
        "df_procesado = pd.concat([df_limpio.reset_index(drop=True), df_info], axis=1)\n",
        "\n",
        "print(\"\\nPrimeras extracciones:\")\n",
        "print(df_procesado[['cargo', 'empresa', 'ubicacion', 'nivel_educativo']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDATFMG8Kjs0"
      },
      "source": [
        "# **Carga y preparaci√≥n de CUOC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epp8AW8kCb-i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "print(\"\\nCargando bases CUOC...\")\n",
        "\n",
        "# =========================================================\n",
        "# 1Ô∏è‚É£ ESTRUCTURA CUOC (una sola columna con c√≥digo y descripci√≥n)\n",
        "# =========================================================\n",
        "try:\n",
        "    df_cuoc_estructura = pd.read_excel(\n",
        "        \"CUOC-Estructura-2024.xlsx\",\n",
        "        header=3,     # salta logo y encabezado visual\n",
        "        usecols=\"A\",  # solo hay una columna\n",
        "        names=[\"texto\"]\n",
        "    )\n",
        "\n",
        "    # Extraer c√≥digo y descripci√≥n (p.ej. \"0110 Oficiales de las Fuerzas Militares\")\n",
        "    df_cuoc_estructura[[\"codigo_cuoc\", \"ocupacion\"]] = (\n",
        "        df_cuoc_estructura[\"texto\"]\n",
        "        .astype(str)\n",
        "        .str.extract(r\"^(\\d+[A-Za-z0-9\\.]*)\\s+(.*)$\")\n",
        "    )\n",
        "\n",
        "    # Limpiar filas sin c√≥digo o sin ocupaci√≥n\n",
        "    df_cuoc_estructura = df_cuoc_estructura.dropna(subset=[\"codigo_cuoc\", \"ocupacion\"])\n",
        "    df_cuoc_estructura = df_cuoc_estructura.reset_index(drop=True)\n",
        "\n",
        "    print(f\"Estructura CUOC cargada: {len(df_cuoc_estructura)} registros\")\n",
        "    print(f\"Columnas: {df_cuoc_estructura.columns.tolist()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå No se pudo cargar CUOC-Estructura-2024.xlsx: {e}\")\n",
        "    df_cuoc_estructura = None\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2Ô∏è‚É£ √çNDICE CUOC (tiene 2 columnas: c√≥digo y descripci√≥n)\n",
        "# =========================================================\n",
        "try:\n",
        "    df_cuoc_indice = pd.read_excel(\n",
        "        \"CUOC-indice-2024.xlsx\",\n",
        "        header=5,  # despu√©s del t√≠tulo \"√çNDICE CUOC 2024\"\n",
        "        usecols=\"B:C\"\n",
        "    )\n",
        "    df_cuoc_indice.columns = [\"codigo_cuoc\", \"ocupacion\"]\n",
        "    df_cuoc_indice = df_cuoc_indice.dropna(subset=[\"codigo_cuoc\", \"ocupacion\"])\n",
        "    df_cuoc_indice = df_cuoc_indice.reset_index(drop=True)\n",
        "\n",
        "    print(f\"√çndice CUOC cargado: {len(df_cuoc_indice)} registros\")\n",
        "    print(f\"Columnas: {df_cuoc_indice.columns.tolist()}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå No se pudo cargar CUOC-indice-2024.xlsx: {e}\")\n",
        "    df_cuoc_indice = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtCZc6vzKpzE"
      },
      "source": [
        "# **Asignaci√≥n de c√≥digo CUOC mediante similitud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dAW_xtaD_0Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalizar_texto_cuoc(texto):\n",
        "    \"\"\"\n",
        "    Normaliza texto para comparaci√≥n CUOC\n",
        "    \"\"\"\n",
        "    if pd.isna(texto):\n",
        "        return \"\"\n",
        "\n",
        "    texto = str(texto).lower()\n",
        "    # Eliminar acentos\n",
        "    texto = ''.join(c for c in unicodedata.normalize('NFD', texto)\n",
        "                    if unicodedata.category(c) != 'Mn')\n",
        "    # Eliminar caracteres especiales\n",
        "    texto = re.sub(r'[^a-z0-9\\s]', ' ', texto)\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "def buscar_codigo_cuoc(cargo, df_cuoc, umbral=60):\n",
        "    \"\"\"\n",
        "    Busca el c√≥digo CUOC m√°s similar al cargo usando fuzzy matching\n",
        "    \"\"\"\n",
        "    if pd.isna(cargo) or cargo == \"\" or df_cuoc is None or len(df_cuoc) == 0:\n",
        "        return {\n",
        "            'codigo_cuoc': None,\n",
        "            'ocupacion_cuoc': None,\n",
        "            'similitud': 0,\n",
        "            'grupo_cuoc': None\n",
        "        }\n",
        "\n",
        "    cargo_norm = normalizar_texto_cuoc(cargo)\n",
        "\n",
        "    # Crear lista de ocupaciones normalizadas\n",
        "    ocupaciones = df_cuoc['ocupacion'].fillna('').tolist()\n",
        "    ocupaciones_norm = [normalizar_texto_cuoc(o) for o in ocupaciones]\n",
        "\n",
        "    # Buscar la mejor coincidencia\n",
        "    mejor_match = process.extractOne(\n",
        "        cargo_norm,\n",
        "        ocupaciones_norm,\n",
        "        scorer=fuzz.token_sort_ratio\n",
        "    )\n",
        "\n",
        "    # Si encontr√≥ una coincidencia v√°lida\n",
        "    if mejor_match and len(mejor_match) >= 2 and mejor_match[1] >= umbral:\n",
        "        texto_match = mejor_match[0]\n",
        "        similitud = mejor_match[1]\n",
        "\n",
        "        # Buscar el √≠ndice de esa coincidencia\n",
        "        try:\n",
        "            idx = ocupaciones_norm.index(texto_match)\n",
        "        except ValueError:\n",
        "            idx = None\n",
        "\n",
        "        if idx is not None:\n",
        "            return {\n",
        "                'codigo_cuoc': df_cuoc.iloc[idx]['codigo_cuoc'],\n",
        "                'ocupacion_cuoc': df_cuoc.iloc[idx]['ocupacion'],\n",
        "                'similitud': similitud,\n",
        "                'grupo_cuoc': df_cuoc.iloc[idx].get('grupo', None)\n",
        "            }\n",
        "\n",
        "    # Si no hay coincidencia suficiente\n",
        "    return {\n",
        "        'codigo_cuoc': None,\n",
        "        'ocupacion_cuoc': None,\n",
        "        'similitud': 0,\n",
        "        'grupo_cuoc': None\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# Asignar c√≥digos CUOC\n",
        "# ============================================\n",
        "\n",
        "print(\"\\nAsignando c√≥digos CUOC a las ofertas...\")\n",
        "\n",
        "if df_cuoc_indice is not None:\n",
        "    # Normalizar columna de ocupaciones en CUOC\n",
        "    df_cuoc_indice['ocupacion_norm'] = df_cuoc_indice['ocupacion'].apply(normalizar_texto_cuoc)\n",
        "\n",
        "    # Aplicar b√∫squeda a cada cargo\n",
        "    cuoc_asignado = df_procesado['cargo'].apply(\n",
        "        lambda x: buscar_codigo_cuoc(x, df_cuoc_indice, umbral=60)\n",
        "    )\n",
        "\n",
        "    # Convertir a DataFrame\n",
        "    df_cuoc_result = pd.DataFrame(cuoc_asignado.tolist())\n",
        "\n",
        "    # Combinar con el dataframe procesado\n",
        "    df_final = pd.concat([df_procesado.reset_index(drop=True), df_cuoc_result], axis=1)\n",
        "\n",
        "    print(\"\\nAsignaciones CUOC exitosas:\")\n",
        "    print(df_final[df_final['codigo_cuoc'].notna()][\n",
        "        ['cargo', 'ocupacion_cuoc', 'codigo_cuoc', 'similitud']\n",
        "    ].head(10))\n",
        "\n",
        "    print(f\"\\nTotal de ofertas con c√≥digo CUOC: {df_final['codigo_cuoc'].notna().sum()}\")\n",
        "    print(f\"Porcentaje de asignaci√≥n: {(df_final['codigo_cuoc'].notna().sum()/len(df_final)*100):.2f}%\")\n",
        "else:\n",
        "    df_final = df_procesado.copy()\n",
        "    print(\"No se pudo realizar asignaci√≥n CUOC (falta archivo)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq9vNrQYK97J"
      },
      "source": [
        "# **An√°lisis descriptivo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_5k4-PIIdpm"
      },
      "outputs": [],
      "source": [
        "def analisis_descriptivo(df):\n",
        "    \"\"\"\n",
        "    Genera an√°lisis descriptivo completo de las ofertas\n",
        "    \"\"\"\n",
        "    analisis = {\n",
        "        'resumen_general': {},\n",
        "        'ocupaciones_frecuentes': {},\n",
        "        'grupos_ocupacionales': {},\n",
        "        'ubicaciones': {},\n",
        "        'nivel_educativo': {},\n",
        "        'experiencia': {},\n",
        "        'salarios': {},\n",
        "        'tipo_contrato': {},\n",
        "        'empresas_top': {},\n",
        "        'tendencia_temporal': {}\n",
        "    }\n",
        "\n",
        "    # RESUMEN GENERAL\n",
        "    analisis['resumen_general'] = {\n",
        "        'total_ofertas': len(df),\n",
        "        'ofertas_con_cargo_identificado': df['cargo'].notna().sum(),\n",
        "        'ofertas_con_cuoc': df['codigo_cuoc'].notna().sum() if 'codigo_cuoc' in df.columns else 0,\n",
        "        'total_vacantes': df['vacantes'].sum() if 'vacantes' in df.columns else 0,\n",
        "        'empresas_unicas': df['empresa'].nunique() if 'empresa' in df.columns else 0\n",
        "    }\n",
        "\n",
        "    # OCUPACIONES M√ÅS FRECUENTES\n",
        "    if 'cargo' in df.columns:\n",
        "        top_cargos = df['cargo'].value_counts().head(20)\n",
        "        analisis['ocupaciones_frecuentes'] = {\n",
        "            str(k): int(v) for k, v in top_cargos.items()\n",
        "        }\n",
        "\n",
        "    # GRUPOS OCUPACIONALES (CUOC)\n",
        "    if 'grupo_cuoc' in df.columns:\n",
        "        grupos = df['grupo_cuoc'].value_counts()\n",
        "        analisis['grupos_ocupacionales'] = {\n",
        "            str(k): int(v) for k, v in grupos.items()\n",
        "        }\n",
        "\n",
        "    # UBICACIONES\n",
        "    if 'ubicacion' in df.columns:\n",
        "        ubicaciones = df['ubicacion'].value_counts()\n",
        "        analisis['ubicaciones'] = {\n",
        "            str(k): int(v) for k, v in ubicaciones.items()\n",
        "        }\n",
        "\n",
        "    # NIVEL EDUCATIVO\n",
        "    if 'nivel_educativo' in df.columns:\n",
        "        niveles = df['nivel_educativo'].value_counts()\n",
        "        analisis['nivel_educativo'] = {\n",
        "            str(k): int(v) for k, v in niveles.items()\n",
        "        }\n",
        "\n",
        "    # EXPERIENCIA\n",
        "    if 'experiencia' in df.columns:\n",
        "        experiencia = df['experiencia'].value_counts()\n",
        "        analisis['experiencia'] = {\n",
        "            str(k): int(v) for k, v in experiencia.items()\n",
        "        }\n",
        "\n",
        "    # TIPO DE CONTRATO\n",
        "    if 'tipo_contrato' in df.columns:\n",
        "        contratos = df['tipo_contrato'].value_counts()\n",
        "        analisis['tipo_contrato'] = {\n",
        "            str(k): int(v) for k, v in contratos.items()\n",
        "        }\n",
        "\n",
        "    # EMPRESAS TOP\n",
        "    if 'empresa' in df.columns:\n",
        "        top_empresas = df['empresa'].value_counts().head(15)\n",
        "        analisis['empresas_top'] = {\n",
        "            str(k): int(v) for k, v in top_empresas.items()\n",
        "        }\n",
        "\n",
        "    # TENDENCIA TEMPORAL\n",
        "    if 'fecha_hora' in df.columns:\n",
        "        df['fecha'] = pd.to_datetime(df['fecha_hora']).dt.date\n",
        "        temporal = df.groupby('fecha').size()\n",
        "        analisis['tendencia_temporal'] = {\n",
        "            str(k): int(v) for k, v in temporal.items()\n",
        "        }\n",
        "\n",
        "    return analisis\n",
        "\n",
        "# Ejecutar an√°lisis\n",
        "print(\"\\nGenerando an√°lisis descriptivo...\")\n",
        "resultados_analisis = analisis_descriptivo(df_final)\n",
        "\n",
        "# Mostrar resumen\n",
        "print(\"\\n=== RESUMEN GENERAL ===\")\n",
        "for key, value in resultados_analisis['resumen_general'].items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n=== TOP 10 OCUPACIONES ===\")\n",
        "for i, (cargo, freq) in enumerate(list(resultados_analisis['ocupaciones_frecuentes'].items())[:10], 1):\n",
        "    print(f\"{i}. {cargo}: {freq} ofertas\")\n",
        "\n",
        "print(\"\\n=== DISTRIBUCI√ìN POR UBICACI√ìN ===\")\n",
        "for ciudad, count in resultados_analisis['ubicaciones'].items():\n",
        "    print(f\"{ciudad}: {count} ofertas\")\n",
        "\n",
        "print(\"\\n=== NIVEL EDUCATIVO ===\")\n",
        "for nivel, count in resultados_analisis['nivel_educativo'].items():\n",
        "    print(f\"{nivel}: {count} ofertas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWef03OBLBHE"
      },
      "source": [
        "# **An√°lisis de salarios**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5TEfWn4I1J4"
      },
      "outputs": [],
      "source": [
        "def extraer_valor_salario(salario_texto):\n",
        "    \"\"\"\n",
        "    Extrae el valor num√©rico del salario\n",
        "    \"\"\"\n",
        "    if pd.isna(salario_texto):\n",
        "        return None\n",
        "\n",
        "    # Eliminar s√≠mbolos y convertir a n√∫mero\n",
        "    numeros = re.findall(r'\\d+', str(salario_texto).replace(',', '').replace('.', ''))\n",
        "\n",
        "    if numeros:\n",
        "        valor = int(''.join(numeros))\n",
        "        # Si el valor es muy peque√±o, puede estar en millones\n",
        "        if valor < 10000:\n",
        "            valor = valor * 1000\n",
        "        return valor\n",
        "\n",
        "    return None\n",
        "\n",
        "def analizar_salarios(df):\n",
        "    \"\"\"\n",
        "    Analiza distribuci√≥n de salarios\n",
        "    \"\"\"\n",
        "    if 'salario' not in df.columns:\n",
        "        return {}\n",
        "\n",
        "    # Extraer valores num√©ricos\n",
        "    df['salario_valor'] = df['salario'].apply(extraer_valor_salario)\n",
        "\n",
        "    # Filtrar valores v√°lidos\n",
        "    salarios_validos = df[df['salario_valor'].notna() & (df['salario_valor'] > 1000000)]\n",
        "\n",
        "    if len(salarios_validos) == 0:\n",
        "        return {'mensaje': 'No hay datos de salarios v√°lidos'}\n",
        "\n",
        "    analisis_sal = {\n",
        "        'total_ofertas_con_salario': len(salarios_validos),\n",
        "        'salario_minimo': int(salarios_validos['salario_valor'].min()),\n",
        "        'salario_maximo': int(salarios_validos['salario_valor'].max()),\n",
        "        'salario_promedio': int(salarios_validos['salario_valor'].mean()),\n",
        "        'salario_mediana': int(salarios_validos['salario_valor'].median()),\n",
        "        'rangos_salariales': {}\n",
        "    }\n",
        "\n",
        "    # Definir rangos salariales (en SMMLV 2025: ~$1,423,500)\n",
        "    smmlv = 1423500\n",
        "    bins = [0, smmlv, 2*smmlv, 3*smmlv, 5*smmlv, float('inf')]\n",
        "    labels = ['<1 SMMLV', '1-2 SMMLV', '2-3 SMMLV', '3-5 SMMLV', '>5 SMMLV']\n",
        "\n",
        "    salarios_validos['rango'] = pd.cut(salarios_validos['salario_valor'], bins=bins, labels=labels)\n",
        "\n",
        "    rangos = salarios_validos['rango'].value_counts()\n",
        "    analisis_sal['rangos_salariales'] = {\n",
        "        str(k): int(v) for k, v in rangos.items()\n",
        "    }\n",
        "\n",
        "    # Salarios por ocupaci√≥n\n",
        "    if 'cargo' in salarios_validos.columns:\n",
        "        sal_por_cargo = salarios_validos.groupby('cargo')['salario_valor'].agg(['mean', 'count'])\n",
        "        sal_por_cargo = sal_por_cargo[sal_por_cargo['count'] >= 2].sort_values('mean', ascending=False)\n",
        "\n",
        "        analisis_sal['salarios_por_ocupacion'] = {\n",
        "            str(k): {\n",
        "                'salario_promedio': int(v['mean']),\n",
        "                'num_ofertas': int(v['count'])\n",
        "            } for k, v in sal_por_cargo.head(10).iterrows()\n",
        "        }\n",
        "\n",
        "    return analisis_sal\n",
        "\n",
        "# Ejecutar an√°lisis de salarios\n",
        "print(\"\\n=== AN√ÅLISIS DE SALARIOS ===\")\n",
        "analisis_salarios_result = analizar_salarios(df_final)\n",
        "\n",
        "if 'mensaje' not in analisis_salarios_result:\n",
        "    print(f\"Ofertas con informaci√≥n salarial: {analisis_salarios_result['total_ofertas_con_salario']}\")\n",
        "    print(f\"Salario promedio: ${analisis_salarios_result['salario_promedio']:,}\")\n",
        "    print(f\"Salario mediana: ${analisis_salarios_result['salario_mediana']:,}\")\n",
        "    print(f\"Rango: ${analisis_salarios_result['salario_minimo']:,} - ${analisis_salarios_result['salario_maximo']:,}\")\n",
        "\n",
        "    print(\"\\n=== DISTRIBUCI√ìN POR RANGOS ===\")\n",
        "    for rango, count in analisis_salarios_result['rangos_salariales'].items():\n",
        "        print(f\"{rango}: {count} ofertas\")\n",
        "else:\n",
        "    print(analisis_salarios_result['mensaje'])\n",
        "\n",
        "# Agregar al an√°lisis principal\n",
        "resultados_analisis['analisis_salarios'] = analisis_salarios_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1oh4c3nLJ9X"
      },
      "source": [
        "# **Exportar resultados a JSON*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQascnNsJbhO"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, date\n",
        "\n",
        "def preparar_para_json(obj):\n",
        "    \"\"\"\n",
        "    Convierte cualquier objeto (NumPy, Pandas, datetime, etc.)\n",
        "    a un tipo compatible con JSON.\n",
        "    \"\"\"\n",
        "    # Tipos num√©ricos\n",
        "    if isinstance(obj, (np.integer, int)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.floating, float)):\n",
        "        return float(obj)\n",
        "\n",
        "    # Fechas y tiempos\n",
        "    elif isinstance(obj, (datetime, date, pd.Timestamp, np.datetime64)):\n",
        "        return str(pd.to_datetime(obj))\n",
        "\n",
        "    # Valores faltantes\n",
        "    elif isinstance(obj, (list, tuple, set, np.ndarray)):\n",
        "        return [preparar_para_json(x) for x in obj]\n",
        "    elif isinstance(obj, dict):\n",
        "        return {str(k): preparar_para_json(v) for k, v in obj.items()}\n",
        "\n",
        "    # Solo aplicar pd.isna a escalares, no arrays\n",
        "    elif isinstance(obj, (str, bool)) or obj is None:\n",
        "        return obj\n",
        "    elif np.isscalar(obj):\n",
        "        if pd.isna(obj):\n",
        "            return None\n",
        "        else:\n",
        "            return obj\n",
        "\n",
        "    return str(obj)  # fallback general\n",
        "\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# EXPORTACI√ìN DE RESULTADOS\n",
        "# =============================================\n",
        "\n",
        "print(\"\\n=== EXPORTANDO RESULTADOS ===\")\n",
        "\n",
        "# Preparar dataset final para JSON\n",
        "df_export = df_final.copy()\n",
        "\n",
        "# Convertir tipos de datos en el DataFrame\n",
        "for col in df_export.columns:\n",
        "    df_export[col] = df_export[col].apply(preparar_para_json)\n",
        "\n",
        "# Crear estructura JSON final\n",
        "resultado_final = {\n",
        "    'metadata': {\n",
        "        'fecha_analisis': datetime.now().isoformat(),\n",
        "        'total_mensajes_originales': int(len(df_mensajes)),\n",
        "        'total_mensajes_procesados': int(len(df_final)),\n",
        "        'fuente': 'Canal WhatsApp - Empleo en Bogot√°'\n",
        "    },\n",
        "    'analisis_descriptivo': preparar_para_json(resultados_analisis),\n",
        "    'ofertas_detalladas': df_export.to_dict(orient='records')\n",
        "}\n",
        "\n",
        "# Guardar JSON\n",
        "with open('analisis_ofertas_empleo.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(preparar_para_json(resultado_final), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úì Archivo JSON guardado: analisis_ofertas_empleo.json\")\n",
        "\n",
        "# Guardar Excel\n",
        "df_export.to_excel('ofertas_procesadas.xlsx', index=False, engine='openpyxl')\n",
        "print(\"‚úì Archivo Excel guardado: ofertas_procesadas.xlsx\")\n",
        "\n",
        "# Guardar resumen CSV\n",
        "df_resumen = df_final[[\n",
        "    'cargo', 'empresa', 'ubicacion', 'salario',\n",
        "    'nivel_educativo', 'experiencia', 'tipo_contrato',\n",
        "    'codigo_cuoc', 'ocupacion_cuoc', 'grupo_cuoc'\n",
        "]].copy()\n",
        "\n",
        "df_resumen.to_csv('resumen_ofertas.csv', index=False, encoding='utf-8-sig')\n",
        "print(\"‚úì Archivo CSV guardado: resumen_ofertas.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETY5Uh5bLP9j"
      },
      "source": [
        "# **Visualizaci√≥n de estad√≠sticas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5cscCYRJpj0"
      },
      "outputs": [],
      "source": [
        "def generar_estadisticas_visuales(resultados):\n",
        "    \"\"\"\n",
        "    Genera un reporte en texto con las estad√≠sticas principales\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"REPORTE FINAL DE AN√ÅLISIS DE OFERTAS LABORALES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Resumen general\n",
        "    print(\"\\nüìä RESUMEN GENERAL\")\n",
        "    print(\"-\" * 60)\n",
        "    rg = resultados['analisis_descriptivo']['resumen_general']\n",
        "    print(f\"Total de ofertas analizadas: {rg['total_ofertas']}\")\n",
        "    print(f\"Ofertas con cargo identificado: {rg['ofertas_con_cargo_identificado']}\")\n",
        "    print(f\"Ofertas clasificadas con CUOC: {rg['ofertas_con_cuoc']}\")\n",
        "    print(f\"Total de vacantes: {rg.get('total_vacantes', 'N/A')}\")\n",
        "    print(f\"Empresas √∫nicas: {rg.get('empresas_unicas', 'N/A')}\")\n",
        "\n",
        "    # Top ocupaciones\n",
        "    print(\"\\nüéØ TOP 10 OCUPACIONES M√ÅS DEMANDADAS\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, (cargo, freq) in enumerate(\n",
        "        list(resultados['analisis_descriptivo']['ocupaciones_frecuentes'].items())[:10], 1\n",
        "    ):\n",
        "        print(f\"{i:2d}. {cargo[:50]:<50} | {freq:>3} ofertas\")\n",
        "\n",
        "    # Grupos ocupacionales\n",
        "    if resultados['analisis_descriptivo']['grupos_ocupacionales']:\n",
        "        print(\"\\nüë• DISTRIBUCI√ìN POR GRUPOS OCUPACIONALES (CUOC)\")\n",
        "        print(\"-\" * 60)\n",
        "        for grupo, count in resultados['analisis_descriptivo']['grupos_ocupacionales'].items():\n",
        "            print(f\"{grupo:<30} | {count:>3} ofertas\")\n",
        "\n",
        "    # Ubicaciones\n",
        "    print(\"\\nüìç DISTRIBUCI√ìN GEOGR√ÅFICA\")\n",
        "    print(\"-\" * 60)\n",
        "    for ciudad, count in resultados['analisis_descriptivo']['ubicaciones'].items():\n",
        "        print(f\"{ciudad:<25} | {count:>3} ofertas\")\n",
        "\n",
        "    # Nivel educativo\n",
        "    print(\"\\nüéì REQUERIMIENTOS EDUCATIVOS\")\n",
        "    print(\"-\" * 60)\n",
        "    for nivel, count in resultados['analisis_descriptivo']['nivel_educativo'].items():\n",
        "        print(f\"{nivel:<25} | {count:>3} ofertas\")\n",
        "\n",
        "    # Salarios\n",
        "    if 'analisis_salarios' in resultados['analisis_descriptivo']:\n",
        "        sal = resultados['analisis_descriptivo']['analisis_salarios']\n",
        "        if 'salario_promedio' in sal:\n",
        "            print(\"\\nüí∞ AN√ÅLISIS SALARIAL\")\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"Salario promedio: ${sal['salario_promedio']:,}\")\n",
        "            print(f\"Salario mediana:  ${sal['salario_mediana']:,}\")\n",
        "            print(f\"Rango salarial:   ${sal['salario_minimo']:,} - ${sal['salario_maximo']:,}\")\n",
        "\n",
        "            print(\"\\nDistribuci√≥n por rangos:\")\n",
        "            for rango, count in sal['rangos_salariales'].items():\n",
        "                print(f\"  {rango:<15} | {count:>3} ofertas\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Fin del reporte\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Generar reporte visual\n",
        "generar_estadisticas_visuales(resultado_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYOKF2y5LYOG"
      },
      "source": [
        "# **Funci√≥n completa integrada**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq5n9d_6KDRu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime, date\n",
        "\n",
        "# --- Funci√≥n para convertir datos a tipos compatibles con JSON ---\n",
        "def preparar_para_json(obj):\n",
        "    \"\"\"\n",
        "    Convierte cualquier objeto (NumPy, Pandas, datetime, etc.)\n",
        "    a un tipo compatible con JSON.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, (np.integer, int)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.floating, float)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, (datetime, date, pd.Timestamp, np.datetime64)):\n",
        "        return str(pd.to_datetime(obj))\n",
        "    elif isinstance(obj, (list, tuple, set, np.ndarray)):\n",
        "        return [preparar_para_json(x) for x in obj]\n",
        "    elif isinstance(obj, dict):\n",
        "        return {str(k): preparar_para_json(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, (str, bool)) or obj is None:\n",
        "        return obj\n",
        "    elif np.isscalar(obj):\n",
        "        if pd.isna(obj):\n",
        "            return None\n",
        "        else:\n",
        "            return obj\n",
        "    return str(obj)\n",
        "\n",
        "# --- Pipeline completo ---\n",
        "def pipeline_completo(ruta_csv, ruta_cuoc_estructura=None, ruta_cuoc_indice=None):\n",
        "    \"\"\"\n",
        "    Funci√≥n que ejecuta todo el pipeline de an√°lisis\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"INICIANDO PIPELINE DE AN√ÅLISIS DE OFERTAS LABORALES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Cargar datos\n",
        "    print(\"\\n[1/9] Cargando datos...\")\n",
        "    df = pd.read_csv(ruta_csv)\n",
        "\n",
        "    # 2. Limpiar textos\n",
        "    print(\"[2/9] Limpiando textos...\")\n",
        "    df['contenido_limpio'] = df['contenido'].apply(limpiar_texto)\n",
        "    df = eliminar_duplicados(df)\n",
        "    df = df[df['contenido_limpio'].str.len() > 50].copy()\n",
        "\n",
        "    # 3. Extraer informaci√≥n\n",
        "    print(\"[3/9] Extrayendo informaci√≥n...\")\n",
        "    info_extraida = df['contenido_limpio'].apply(extraer_informacion_oferta)\n",
        "    df_info = pd.DataFrame(info_extraida.tolist())\n",
        "    df_procesado = pd.concat([df.reset_index(drop=True), df_info], axis=1)\n",
        "\n",
        "    # 4. Cargar CUOC\n",
        "    print(\"[4/9] Cargando bases CUOC...\")\n",
        "    df_cuoc = None\n",
        "    if ruta_cuoc_indice:\n",
        "        try:\n",
        "            df_cuoc = pd.read_excel(ruta_cuoc_indice)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† No se pudo cargar CUOC: {e}\")\n",
        "\n",
        "    # 5. Asignar c√≥digos CUOC\n",
        "    print(\"[5/9] Asignando c√≥digos CUOC...\")\n",
        "    if df_cuoc is not None:\n",
        "        cuoc_asignado = df_procesado['cargo'].apply(\n",
        "            lambda x: buscar_codigo_cuoc(x, df_cuoc, umbral=60)\n",
        "        )\n",
        "        df_cuoc_result = pd.DataFrame(cuoc_asignado.tolist())\n",
        "        df_final = pd.concat([df_procesado.reset_index(drop=True), df_cuoc_result], axis=1)\n",
        "    else:\n",
        "        df_final = df_procesado.copy()\n",
        "\n",
        "    # 6. An√°lisis descriptivo\n",
        "    print(\"[6/9] Generando an√°lisis descriptivo...\")\n",
        "    analisis = analisis_descriptivo(df_final)\n",
        "\n",
        "    # 7. An√°lisis de salarios\n",
        "    print(\"[7/9] Analizando salarios...\")\n",
        "    analisis['analisis_salarios'] = analizar_salarios(df_final)\n",
        "\n",
        "    # 8. Preparar JSON\n",
        "    print(\"[8/9] Preparando resultados...\")\n",
        "    resultado_json = {\n",
        "        'metadata': {\n",
        "            'fecha_analisis': datetime.now().isoformat(),\n",
        "            'total_mensajes_originales': len(df),\n",
        "            'total_mensajes_procesados': len(df_final)\n",
        "        },\n",
        "        'analisis_descriptivo': analisis,\n",
        "        'ofertas_detalladas': df_final.to_dict(orient='records')\n",
        "    }\n",
        "\n",
        "    # 9. Exportar\n",
        "    print(\"[9/9] Exportando resultados...\")\n",
        "    with open('analisis_ofertas_empleo.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(preparar_para_json(resultado_json), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    df_final.to_excel('ofertas_procesadas.xlsx', index=False)\n",
        "\n",
        "    print(\"\\n‚úì Pipeline completado exitosamente\")\n",
        "    generar_estadisticas_visuales(resultado_json)\n",
        "\n",
        "    return df_final, resultado_json\n",
        "\n",
        "# === EJECUTAR PIPELINE COMPLETO ===\n",
        "df_resultado, json_resultado = pipeline_completo(\n",
        "    ruta_csv='mensajes.csv',\n",
        "    ruta_cuoc_indice='CUOC_Indice_2024.xlsx'\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}